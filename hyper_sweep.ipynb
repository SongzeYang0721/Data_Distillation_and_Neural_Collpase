{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e0e813",
   "metadata": {},
   "source": [
    "# Hyperparameter Sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c54880c",
   "metadata": {},
   "source": [
    "This notebook implements an hyperparameter sweep for data distillation algorithm utilizing the recent finding in neural collapse. \n",
    "\n",
    "The main papers considered here are \n",
    " - data distillation:\n",
    "     - https://github.com/SsnL/dataset-distillation \n",
    "     - https://github.com/google-research/google-research/tree/master/kip\n",
    " - Neural Collapse:\n",
    "     - https://github.com/tding1/Neural-Collapse. \n",
    "\n",
    "The neural network has the option to fix the last layer weight matrix to be a simplex ETF. The ETF is a benign optimization landscape empeirically observed in practice as long as the network enters its terminal phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768c8e0",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45618fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myangsongze2008\u001b[0m (\u001b[33mdata-distillation-with-nc\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd64322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5aace5",
   "metadata": {},
   "source": [
    "Let's import the file from the https://github.com/tding1/Neural-Collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988e1e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "from models.res_adapt import ResNet18_adapt\n",
    "from utils import *\n",
    "from args import parse_train_args\n",
    "from data.datasets import make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b8382fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "from models.resnet import ResNet, BasicBlock\n",
    "\n",
    "from utils import *\n",
    "from train_2nd_order import weight_decay, trainer\n",
    "from validate_NC import compute_Wh_b_relation, compute_W_H_relation, compute_ETF, compute_Sigma_B, compute_Sigma_W,compute_info,FCFeatures\n",
    "\n",
    "from data.datasets import make_dataset\n",
    "from arg_loader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14274f5",
   "metadata": {},
   "source": [
    "# Load Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1c9e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture params\n",
    "model='resnet18'\n",
    "bias=True\n",
    "ETF_fc=True\n",
    "fixdim=0\n",
    "SOTA=False\n",
    "\n",
    "# MLP settings (only when using mlp and res_adapt(in which case only width has effect))\n",
    "width=1024\n",
    "depth=6\n",
    "\n",
    "# hardware settings\n",
    "gpu_id=0\n",
    "seed=6\n",
    "use_cudnn=True\n",
    "\n",
    "# dataset\n",
    "dataset='cifar10'\n",
    "data_dir='~/data'\n",
    "uid=\"tmp\"\n",
    "force=True\n",
    "\n",
    "# learning options\n",
    "epochs = 200\n",
    "batch_size=2048\n",
    "loss = 'CrossEntropy'\n",
    "sample_size = None\n",
    "\n",
    "# optimization\n",
    "lr=0.001\n",
    "optimizer = \"Adam\"\n",
    "history_size=10\n",
    "device = \"cpu\"\n",
    "check = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b3e386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "override this uidtmp\n",
      "<arg_loader.train_args object at 0x2812abd90>\n",
      "cudnn is used\n"
     ]
    }
   ],
   "source": [
    "args = train_args(model=model,bias=bias,ETF_fc=ETF_fc,fixdim=fixdim,SOTA=SOTA,\n",
    "                  width=width,depth=depth,\n",
    "                  gpu_id=gpu_id,seed=seed,use_cudnn=use_cudnn,\n",
    "                  dataset=dataset,data_dir=data_dir,uid=uid,force=force,\n",
    "                  epochs=epochs,batch_size = batch_size,loss = loss,sample_size=sample_size,\n",
    "                  lr = lr,optimizer=optimizer,history_size=history_size, \n",
    "                  device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f27ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "if check:\n",
    "    torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45197578",
   "metadata": {},
   "source": [
    "# Define function to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b50e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(args, model, trainloader, epoch_id, criterion, optimizer):\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "\n",
    "        inputs, targets = inputs.to(args.device), targets.to(args.device)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        def closure():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if args.loss == 'CrossEntropy':\n",
    "                loss = criterion(outputs[0], targets) + weight_decay(args, model)\n",
    "            elif args.loss == 'MSE':\n",
    "                loss = criterion(outputs[0], nn.functional.one_hot(targets).type(torch.FloatTensor).to(args.device)) \\\n",
    "                       + weight_decay(args, model)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        model.eval()\n",
    "        outputs = model(inputs)\n",
    "        prec1, prec5 = compute_accuracy(outputs[0].data, targets.data, topk=(1, 5))\n",
    "\n",
    "        if args.loss == 'CrossEntropy':\n",
    "            loss = criterion(outputs[0], targets) + weight_decay(args, model)\n",
    "        elif args.loss == 'MSE':\n",
    "            loss = criterion(outputs[0], nn.functional.one_hot(targets).type(torch.FloatTensor).to(args.device)) \\\n",
    "                   + weight_decay(args, model)\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # if batch_idx % 10 == 0:\n",
    "    print('[epoch: %d] (%d/%d) | Loss: %.4f | top1: %.4f | top5: %.4f ' %\n",
    "          (epoch_id + 1, batch_idx + 1, len(trainloader), losses.avg, top1.avg, top5.avg))\n",
    "                \n",
    "    \n",
    "            # wandb.log({\n",
    "            #     \"losses.avg\":losses.avg, \n",
    "            #     \"top1.avg\":top1.avg,\n",
    "            #     \"top5.avg\":top5.avg\n",
    "            # })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "273ec6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_NC(args,model,testloader):\n",
    "    \n",
    "    args.load_path = \"model_weights/tmp/\"\n",
    "\n",
    "    if args.load_path is None:\n",
    "        sys.exit('Need to input the path to a pre-trained model!')\n",
    "\n",
    "    fc_features = FCFeatures()\n",
    "    model.fc.register_forward_pre_hook(fc_features)\n",
    "    info_dict = {\n",
    "            'collapse_metric': [],\n",
    "            'ETF_metric': [],\n",
    "            'WH_relation_metric': [],\n",
    "            'Wh_b_relation_metric': [],\n",
    "            'W': [],\n",
    "            'b': [],\n",
    "            'H': [],\n",
    "            'mu_G_train': [],\n",
    "            # 'mu_G_test': [],\n",
    "            'train_acc1': [],\n",
    "            'train_acc5': [],\n",
    "            'test_acc1': [],\n",
    "            'test_acc5': []\n",
    "        }\n",
    "\n",
    "    logfile = open('%s/test_log.txt' % (args.load_path), 'w')\n",
    "    for i in range(args.epochs):\n",
    "\n",
    "        model.load_state_dict(torch.load(args.load_path + 'epoch_' + str(i + 1).zfill(3) + '.pth'))\n",
    "        model.eval()\n",
    "\n",
    "        for n, p in model.named_parameters():\n",
    "            if 'fc.weight' in n:\n",
    "                W = p\n",
    "            if 'fc.bias' in n:\n",
    "                b = p\n",
    "\n",
    "        mu_G_train, mu_c_dict_train, train_acc1, train_acc5 = compute_info(args, model, fc_features, trainloader, isTrain=True)\n",
    "        mu_G_test, mu_c_dict_test, test_acc1, test_acc5 = compute_info(args, model, fc_features, testloader, isTrain=False)\n",
    "\n",
    "        Sigma_W = compute_Sigma_W(args, model, fc_features, mu_c_dict_train, trainloader, isTrain=True)\n",
    "        # Sigma_W_test_norm = compute_Sigma_W(args, model, fc_features, mu_c_dict_train, testloader, isTrain=False)\n",
    "        Sigma_B = compute_Sigma_B(mu_c_dict_train, mu_G_train)\n",
    "\n",
    "        collapse_metric = np.trace(Sigma_W @ scilin.pinv(Sigma_B)) / len(mu_c_dict_train)\n",
    "        ETF_metric = compute_ETF(W)\n",
    "        WH_relation_metric, H = compute_W_H_relation(W, mu_c_dict_train, mu_G_train)\n",
    "        if args.bias:\n",
    "            Wh_b_relation_metric = compute_Wh_b_relation(W, mu_G_train, b)\n",
    "        else:\n",
    "            Wh_b_relation_metric = compute_Wh_b_relation(W, mu_G_train, torch.zeros((W.shape[0], )))\n",
    "\n",
    "        info_dict['collapse_metric'].append(collapse_metric)\n",
    "        info_dict['ETF_metric'].append(ETF_metric)\n",
    "        info_dict['WH_relation_metric'].append(WH_relation_metric)\n",
    "        info_dict['Wh_b_relation_metric'].append(Wh_b_relation_metric)\n",
    "\n",
    "        info_dict['W'].append((W.detach().cpu().numpy()))\n",
    "        if args.bias:\n",
    "            info_dict['b'].append(b.detach().cpu().numpy())\n",
    "        info_dict['H'].append(H.detach().cpu().numpy())\n",
    "\n",
    "        info_dict['mu_G_train'].append(mu_G_train.detach().cpu().numpy())\n",
    "        # info_dict['mu_G_test'].append(mu_G_test.detach().cpu().numpy())\n",
    "\n",
    "        info_dict['train_acc1'].append(train_acc1)\n",
    "        info_dict['train_acc5'].append(train_acc5)\n",
    "        info_dict['test_acc1'].append(test_acc1)\n",
    "        info_dict['test_acc5'].append(test_acc5)\n",
    "\n",
    "        \n",
    "\n",
    "    print_and_save('[epoch: %d] | train top1: %.4f | train top5: %.4f | test top1: %.4f | test top5: %.4f ' %\n",
    "                    (i + 1, train_acc1, train_acc5, test_acc1, test_acc5), logfile)\n",
    "    \n",
    "    wandb.log({\n",
    "                   \"train_acc1\":train_acc1, \n",
    "                   \"train_acc5\":train_acc5,\n",
    "                   \"test_acc1\":test_acc1,\n",
    "                   \"test_acc5\":test_acc5\n",
    "                })\n",
    "    \n",
    "    wandb.log({\"collapse_metric\":collapse_metric, \n",
    "                   \"ETF_metric\":ETF_metric, \n",
    "                   \"WH_relation_metric\":WH_relation_metric,\n",
    "                   \"Wh_b_relation_metric\":Wh_b_relation_metric\n",
    "                })\n",
    "\n",
    "\n",
    "    with open(args.load_path + 'info.pkl', 'wb') as f:\n",
    "        pickle.dump(info_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cfb2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluater_NC(args,model,testloader,fc_features,info_dict):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'fc.weight' in n:\n",
    "            W = p\n",
    "        if 'fc.bias' in n:\n",
    "            b = p\n",
    "\n",
    "    mu_G_train, mu_c_dict_train, train_acc1, train_acc5 = compute_info(args, model, fc_features, trainloader, isTrain=True)\n",
    "    mu_G_test, mu_c_dict_test, test_acc1, test_acc5 = compute_info(args, model, fc_features, testloader, isTrain=False)\n",
    "\n",
    "    Sigma_W = compute_Sigma_W(args, model, fc_features, mu_c_dict_train, trainloader, isTrain=True)\n",
    "    # Sigma_W_test_norm = compute_Sigma_W(args, model, fc_features, mu_c_dict_train, testloader, isTrain=False)\n",
    "    Sigma_B = compute_Sigma_B(mu_c_dict_train, mu_G_train)\n",
    "\n",
    "    collapse_metric = np.trace(Sigma_W @ scilin.pinv(Sigma_B)) / len(mu_c_dict_train)\n",
    "    ETF_metric = compute_ETF(W)\n",
    "    WH_relation_metric, H = compute_W_H_relation(W, mu_c_dict_train, mu_G_train)\n",
    "    if args.bias:\n",
    "        Wh_b_relation_metric = compute_Wh_b_relation(W, mu_G_train, b)\n",
    "    else:\n",
    "        Wh_b_relation_metric = compute_Wh_b_relation(W, mu_G_train, torch.zeros((W.shape[0], )))\n",
    "\n",
    "    info_dict['collapse_metric'].append(collapse_metric)\n",
    "    info_dict['ETF_metric'].append(ETF_metric)\n",
    "    info_dict['WH_relation_metric'].append(WH_relation_metric)\n",
    "    info_dict['Wh_b_relation_metric'].append(Wh_b_relation_metric)\n",
    "\n",
    "    info_dict['W'].append((W.detach().cpu().numpy()))\n",
    "    if args.bias:\n",
    "        info_dict['b'].append(b.detach().cpu().numpy())\n",
    "    info_dict['H'].append(H.detach().cpu().numpy())\n",
    "\n",
    "    info_dict['mu_G_train'].append(mu_G_train.detach().cpu().numpy())\n",
    "    # info_dict['mu_G_test'].append(mu_G_test.detach().cpu().numpy())\n",
    "\n",
    "    info_dict['train_acc1'].append(train_acc1)\n",
    "    info_dict['train_acc5'].append(train_acc5)\n",
    "    info_dict['test_acc1'].append(test_acc1)\n",
    "    info_dict['test_acc5'].append(test_acc5)\n",
    "\n",
    "        \n",
    "\n",
    "    print_and_save('[epoch: %d] | train top1: %.4f | train top5: %.4f | test top1: %.4f | test top5: %.4f ' %\n",
    "                    (i + 1, train_acc1, train_acc5, test_acc1, test_acc5), logfile)\n",
    "    \n",
    "#     wandb.log({\n",
    "#                    \"train_acc1\":train_acc1, \n",
    "#                    \"train_acc5\":train_acc5,\n",
    "#                    \"test_acc1\":test_acc1,\n",
    "#                    \"test_acc5\":test_acc5\n",
    "#                 })\n",
    "    \n",
    "#     wandb.log({\"collapse_metric\":collapse_metric, \n",
    "#                    \"ETF_metric\":ETF_metric, \n",
    "#                    \"WH_relation_metric\":WH_relation_metric,\n",
    "#                    \"Wh_b_relation_metric\":Wh_b_relation_metric\n",
    "#                 })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d33b0d",
   "metadata": {},
   "source": [
    "# Set up a sweep configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\" : \"random\"\n",
    "}\n",
    "\n",
    "metric = {\n",
    "    \"name\": \"loss\",\n",
    "    \"goal\": \"minimize\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f6aa1",
   "metadata": {},
   "source": [
    "Set up parameters to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2028abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    \"learning_rate\":{\n",
    "        \"values\": [0.1,0.05,0.001]\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"values\": [\"Adam\",\"SGD\",\"LBFGS\"]\n",
    "    },\n",
    "#     \"width\":{\n",
    "#         \"values\": [1024,2048,4096]\n",
    "#     },\n",
    "#     \"depth\":{\n",
    "#         \"values\": [4,8,12]\n",
    "#     },\n",
    "    \"batch_size\":{\n",
    "        \"values\":[32,64,256,2048]\n",
    "    }\n",
    "}\n",
    "\n",
    "parameters_dict.update({\n",
    "    \"model\":{\n",
    "        \"values\": [\"ResNet18_adapt\"]\n",
    "    },\n",
    "    \"epochs\":{\n",
    "        \"value\": 200\n",
    "    },\n",
    "    \"loss\":{\n",
    "        \"value\":'CrossEntropy'\n",
    "    }\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d84186",
   "metadata": {},
   "source": [
    "# Start the sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b8326",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config[\"metric\"]=metric\n",
    "sweep_config[\"parameters\"]=parameters_dict\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"hyper_sweep_4_opt_para\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(arg):\n",
    "    \n",
    "    trainloader, testloader, num_classes = make_dataset(arg.dataset, \n",
    "                                           arg.data_dir, \n",
    "                                           arg.batch_size, \n",
    "                                           SOTA=arg.SOTA)\n",
    "    \n",
    "    if args.model == \"MLP\":\n",
    "        model = models.__dict__[args.model](hidden = args.width, depth = args.depth, fc_bias=args.bias, num_classes=num_classes).to(args.device)\n",
    "    elif args.model == \"ResNet18_adapt\":\n",
    "        model = ResNet18_adapt(width = args.width, num_classes=num_classes, fc_bias=args.bias).to(args.device)\n",
    "    else:\n",
    "        model = models.__dict__[args.model](num_classes=num_classes, fc_bias=args.bias, ETF_fc=args.ETF_fc, fixdim=args.fixdim, SOTA=args.SOTA).to(args.device)\n",
    "\n",
    "    print('# of model parameters: ' + str(count_network_parameters(model)))\n",
    "    print(type(model))\n",
    "    \n",
    "    criterion = make_criterion(args)\n",
    "    optimizer = make_optimizer(args, model)\n",
    "    \n",
    "    fc_features = FCFeatures()\n",
    "    model.fc.register_forward_pre_hook(fc_features)\n",
    "    info_dict = {\n",
    "            'collapse_metric': [],\n",
    "            'ETF_metric': [],\n",
    "            'WH_relation_metric': [],\n",
    "            'Wh_b_relation_metric': [],\n",
    "            'W': [],\n",
    "            'b': [],\n",
    "            'H': [],\n",
    "            'mu_G_train': [],\n",
    "            # 'mu_G_test': [],\n",
    "            'train_acc1': [],\n",
    "            'train_acc5': [],\n",
    "            'test_acc1': [],\n",
    "            'test_acc5': []\n",
    "        }\n",
    "\n",
    "\n",
    "    for epoch_id in range(args.epochs):\n",
    "        trainer(args, model, trainloader, epoch_id, criterion, optimizer)\n",
    "        evaluater_NC(args,model,testloader,fc_features,info_dict)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84024096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_validate(config = None, args = args):\n",
    "    # Initial a new run\n",
    "    with wandb.init(config=config):\n",
    "        print(\"Initialise finished, starting now...\")\n",
    "        config = wandb.config\n",
    "        \n",
    "        trainloader, testloader, num_classes = make_dataset(args.dataset,\n",
    "                                                            args.data_dir,\n",
    "                                                            config.batch_size,\n",
    "                                                            SOTA=args.SOTA)\n",
    "        images, labels = next(iter(trainloader))\n",
    "        size_train, channels, height, width = images.shape\n",
    "        nc = channels\n",
    "        input_size = height, width\n",
    "        print(\"The number of class in our training set is \", num_classes)\n",
    "        print(\"Batch size:\", size_train, \"Number of channels:\", channels, \"input height:\", height, \"input width:\", width)\n",
    "        \n",
    "        if config.model == \"MLP\":\n",
    "            model = models.__dict__[config.model](hidden = config.width, depth = config.depth, fc_bias=args.bias, num_classes=num_classes).to(args.device)\n",
    "        elif config.model == \"ResNet18_adapt\":\n",
    "            model = ResNet18_adapt(width = config.width, num_classes=num_classes, fc_bias=args.bias).to(args.device)\n",
    "        else:\n",
    "            model = models.__dict__[config.model](num_classes=num_classes, fc_bias=args.bias, ETF_fc=args.ETF_fc, fixdim=args.fixdim, SOTA=args.SOTA).to(args.device)\n",
    "\n",
    "        print('# of model parameters: ' + str(count_network_parameters(model)))\n",
    "        print(type(model))\n",
    "        args.optimizer = config.optimizer\n",
    "        args.lr = config.learning_rate\n",
    "        train(args, model, trainloader)\n",
    "        evaluate_NC(args, model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38114b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b40168",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_validate(config = sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ad95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900332e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id,function=train_n_validate,count=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
